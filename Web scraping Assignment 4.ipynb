{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7659928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing librarires\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "\n",
    "# importing requests\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9225416",
   "metadata": {},
   "source": [
    "### 1.Scrape the details of most viewed videos on YouTube from Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09acd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7f0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching url\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "395bd676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Video Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Views in Billion</th>\n",
       "      <th>Upload Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>14.09</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.38</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[17]</td>\n",
       "      <td>LooLoo Kids - Nursery Rhymes and Children's Songs</td>\n",
       "      <td>6.87</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Bath Song\"[18]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>6.62</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Shape of You\"[19]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.20</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>\"See You Again\"[22]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>6.17</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>\"Wheels on the Bus\"[27]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>5.88</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>\"Phonics Song with Two Words\"[28]</td>\n",
       "      <td>ChuChu TV Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>5.70</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>\"Uptown Funk\"[29]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>5.15</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[30]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>5.07</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>\"Gangnam Style\"[31]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>5.05</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[36]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.58</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>\"Dame Tu Cosita\"[37]</td>\n",
       "      <td>Ultra Records</td>\n",
       "      <td>4.55</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>\"Axel F\"[38]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>4.34</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>\"Sugar\"[39]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.97</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>3.96</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>\"Roar\"[42]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.96</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[43]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.91</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[44]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.85</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>\"Sorry\"[45]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.77</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>\"Thinking Out Loud\"[46]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.73</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[47]</td>\n",
       "      <td>Kiddiestv Hindi - Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.73</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>\"Shree Hanuman Chalisa\"[48]</td>\n",
       "      <td>T-Series Bhakti Sagar</td>\n",
       "      <td>3.69</td>\n",
       "      <td>May 10, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>\"Dark Horse\"[49]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.67</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>\"Perfect\"[50]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.67</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>\"Let Her Go\"[51]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.61</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>\"Faded\"[52]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.59</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>\"Girls Like You\"[53]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.56</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>\"Lean On\"[54]</td>\n",
       "      <td>Major Lazer Official</td>\n",
       "      <td>3.55</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                       Video Name  \\\n",
       "0      1                            \"Baby Shark Dance\"[6]   \n",
       "1      2                                   \"Despacito\"[9]   \n",
       "2      3                       \"Johny Johny Yes Papa\"[17]   \n",
       "3      4                                  \"Bath Song\"[18]   \n",
       "4      5                               \"Shape of You\"[19]   \n",
       "5      6                              \"See You Again\"[22]   \n",
       "6      7                          \"Wheels on the Bus\"[27]   \n",
       "7      8                \"Phonics Song with Two Words\"[28]   \n",
       "8      9                                \"Uptown Funk\"[29]   \n",
       "9     10  \"Learning Colors – Colorful Eggs on a Farm\"[30]   \n",
       "10    11                              \"Gangnam Style\"[31]   \n",
       "11    12   \"Masha and the Bear – Recipe for Disaster\"[36]   \n",
       "12    13                             \"Dame Tu Cosita\"[37]   \n",
       "13    14                                     \"Axel F\"[38]   \n",
       "14    15                                      \"Sugar\"[39]   \n",
       "15    16                             \"Counting Stars\"[40]   \n",
       "16    17                        \"Baa Baa Black Sheep\"[41]   \n",
       "17    18                                       \"Roar\"[42]   \n",
       "18    19                             \"Lakdi Ki Kathi\"[43]   \n",
       "19    20           \"Waka Waka (This Time for Africa)\"[44]   \n",
       "20    21                                      \"Sorry\"[45]   \n",
       "21    22                          \"Thinking Out Loud\"[46]   \n",
       "22    23          \"Humpty the train on a fruits ride\"[47]   \n",
       "23    24                      \"Shree Hanuman Chalisa\"[48]   \n",
       "24    25                                 \"Dark Horse\"[49]   \n",
       "25    26                                    \"Perfect\"[50]   \n",
       "26    27                                 \"Let Her Go\"[51]   \n",
       "27    28                                      \"Faded\"[52]   \n",
       "28    29                             \"Girls Like You\"[53]   \n",
       "29    30                                    \"Lean On\"[54]   \n",
       "\n",
       "                                               Artist Views in Billion  \\\n",
       "0         Pinkfong Baby Shark - Kids' Songs & Stories            14.09   \n",
       "1                                          Luis Fonsi             8.38   \n",
       "2   LooLoo Kids - Nursery Rhymes and Children's Songs             6.87   \n",
       "3                          Cocomelon - Nursery Rhymes             6.62   \n",
       "4                                          Ed Sheeran             6.20   \n",
       "5                                         Wiz Khalifa             6.17   \n",
       "6                          Cocomelon - Nursery Rhymes             5.88   \n",
       "7               ChuChu TV Nursery Rhymes & Kids Songs             5.70   \n",
       "8                                         Mark Ronson             5.15   \n",
       "9                                         Miroshka TV             5.07   \n",
       "10                                                Psy             5.05   \n",
       "11                                         Get Movies             4.58   \n",
       "12                                      Ultra Records             4.55   \n",
       "13                                         Crazy Frog             4.34   \n",
       "14                                           Maroon 5             4.00   \n",
       "15                                        OneRepublic             3.97   \n",
       "16                         Cocomelon - Nursery Rhymes             3.96   \n",
       "17                                         Katy Perry             3.96   \n",
       "18                                       Jingle Toons             3.91   \n",
       "19                                            Shakira             3.85   \n",
       "20                                      Justin Bieber             3.77   \n",
       "21                                         Ed Sheeran             3.73   \n",
       "22      Kiddiestv Hindi - Nursery Rhymes & Kids Songs             3.73   \n",
       "23                              T-Series Bhakti Sagar             3.69   \n",
       "24                                         Katy Perry             3.67   \n",
       "25                                         Ed Sheeran             3.67   \n",
       "26                                          Passenger             3.61   \n",
       "27                                        Alan Walker             3.59   \n",
       "28                                           Maroon 5             3.56   \n",
       "29                               Major Lazer Official             3.55   \n",
       "\n",
       "          Upload Date  \n",
       "0       June 17, 2016  \n",
       "1    January 12, 2017  \n",
       "2     October 8, 2016  \n",
       "3         May 2, 2018  \n",
       "4    January 30, 2017  \n",
       "5       April 6, 2015  \n",
       "6        May 24, 2018  \n",
       "7       March 6, 2014  \n",
       "8   November 19, 2014  \n",
       "9   February 27, 2018  \n",
       "10      July 15, 2012  \n",
       "11   January 31, 2012  \n",
       "12      April 5, 2018  \n",
       "13      June 16, 2009  \n",
       "14   January 14, 2015  \n",
       "15       May 31, 2013  \n",
       "16      June 25, 2018  \n",
       "17  September 5, 2013  \n",
       "18      June 14, 2018  \n",
       "19       June 4, 2010  \n",
       "20   October 22, 2015  \n",
       "21    October 7, 2014  \n",
       "22   January 26, 2018  \n",
       "23       May 10, 2011  \n",
       "24  February 20, 2014  \n",
       "25   November 9, 2017  \n",
       "26      July 25, 2012  \n",
       "27   December 3, 2015  \n",
       "28       May 31, 2018  \n",
       "29     March 22, 2015  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank = []\n",
    "name = []\n",
    "views = []\n",
    "artist = []\n",
    "upload_date = []\n",
    "\n",
    "# for scraping the name on the page\n",
    "name_tags = driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[1]')\n",
    "for i in name_tags:\n",
    "    naming = i.text\n",
    "    name.append(naming)\n",
    "    \n",
    "# for scraping the artist on the page\n",
    "artist_tags = driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[2]')\n",
    "for i in artist_tags:\n",
    "    uploader = i.text\n",
    "    artist.append(uploader)\n",
    "    \n",
    "# for scraping the views on respective videos\n",
    "view_tags = driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[3]')\n",
    "for i in view_tags:\n",
    "    view_no = i.text\n",
    "    views.append(view_no)\n",
    "    \n",
    "# for scraping the upload date of respective videos\n",
    "date_tags = driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[4]')\n",
    "for i in date_tags:\n",
    "    date = i.text\n",
    "    upload_date.append(date)\n",
    "    \n",
    "#  Finally creating a data frame of the scraped data\n",
    "import pandas as pd\n",
    "df= pd.DataFrame({'Video Name':name,'Artist':artist,\"Views in Billion\":views,'Upload Date':upload_date})\n",
    "df.index=df.index+1\n",
    "df=df.reset_index()\n",
    "df=df.rename(columns={\"index\":\"Rank\"})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca06b0",
   "metadata": {},
   "source": [
    "### 2.Scrape the details team India’s international fixtures from bcci.tv.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d528a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Fetching url\n",
    "driver.get('http://www.bcci.tv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9003b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixture_tab= driver.find_element(By.XPATH,'/html/body/header/div[3]/div[2]/ul/div[1]/a[2]')\n",
    "fixture_tab.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08b50902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "      <td>Himachal Pradesh Cricket Association Stadium, ...</td>\n",
       "      <td>7 MARCH, 2024</td>\n",
       "      <td>9:30 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>6 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>7 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>10 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>13 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "      <td>Harare Sports Club, Harare</td>\n",
       "      <td>14 JULY, 2024</td>\n",
       "      <td>8:00 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Series  \\\n",
       "0  ENGLAND TOUR OF INDIA 2023-24   \n",
       "1    INDIA TOUR OF ZIMBABWE 2024   \n",
       "2    INDIA TOUR OF ZIMBABWE 2024   \n",
       "3    INDIA TOUR OF ZIMBABWE 2024   \n",
       "4    INDIA TOUR OF ZIMBABWE 2024   \n",
       "5    INDIA TOUR OF ZIMBABWE 2024   \n",
       "\n",
       "                                               Place           Date  \\\n",
       "0  Himachal Pradesh Cricket Association Stadium, ...  7 MARCH, 2024   \n",
       "1                         Harare Sports Club, Harare   6 JULY, 2024   \n",
       "2                         Harare Sports Club, Harare   7 JULY, 2024   \n",
       "3                         Harare Sports Club, Harare  10 JULY, 2024   \n",
       "4                         Harare Sports Club, Harare  13 JULY, 2024   \n",
       "5                         Harare Sports Club, Harare  14 JULY, 2024   \n",
       "\n",
       "          Time  \n",
       "0  9:30 AM IST  \n",
       "1  8:00 PM IST  \n",
       "2  8:00 PM IST  \n",
       "3  8:00 PM IST  \n",
       "4  8:00 PM IST  \n",
       "5  8:00 PM IST  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the empty list\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []\n",
    "\n",
    "# for scraping the series \n",
    "series_tags = driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "for i in series_tags:\n",
    "    series= i.text\n",
    "    Series.append(series)\n",
    "\n",
    "# for scraping the place \n",
    "place_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-place ng-scope\"]')\n",
    "for i in place_tags:\n",
    "    place= i.text\n",
    "    Place.append(place)\n",
    "    \n",
    "# for scraping the Dates\n",
    "date_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "for i in date_tags:\n",
    "    date= i.text\n",
    "    Date.append(date)\n",
    "    \n",
    "# for scraping the time \n",
    "time_tags = driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "for i in time_tags:\n",
    "    time= i.text\n",
    "    Time.append(time)\n",
    "    \n",
    "# Creating the data frame\n",
    "import pandas as pd \n",
    "df= pd.DataFrame({'Series':Series,'Place':Place,\"Date\":Date,'Time':Time})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a8f31",
   "metadata": {},
   "source": [
    "### 3. Scrape the details of State-wise GDP of India from statisticstime.com\n",
    "Url = http://statisticstimes.com/\n",
    "\n",
    "You have to find following details: \n",
    "    A) Rank\n",
    "    B) State\n",
    "    C) GSDP(18-19)- at current prices\n",
    "    D) GSDP(19-20)- at current prices\n",
    "    E) Share(18-19)\n",
    "    F) GDP($ billion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e683a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get(\"http://statisticstimes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5a07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_link=driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/div[1]/a[3]')\n",
    "try:\n",
    "    country_link.click()\n",
    "except ElementNotInteractableException:#handling element not clickable exception\n",
    "    driver.get(country_link.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e55f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "link=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a296ebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n"
     ]
    }
   ],
   "source": [
    "#scrape rank\n",
    "rank=[]\n",
    "try:\n",
    "    ranks=driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"][1]/tbody/tr/td[1]')\n",
    "    for x in ranks:\n",
    "        rank.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    rank.append('-')\n",
    "rank=rank[0:33]\n",
    "print(len(rank),rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da5e3a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['Maharashtra', 'Tamil Nadu', 'Uttar Pradesh', 'Karnataka', 'Gujarat', 'West Bengal', 'Rajasthan', 'Madhya Pradesh', 'Andhra Pradesh', 'Telangana', 'Kerala', 'Delhi', 'Haryana', 'Odisha', 'Bihar', 'Punjab', 'Assam', 'Chhattisgarh', 'Jharkhand', 'Uttarakhand', 'Jammu & Kashmir-UT', 'Himachal Pradesh', 'Goa', 'Tripura', 'Chandigarh', 'Puducherry', 'Meghalaya', 'Sikkim', 'Manipur', 'Arunachal Pradesh', 'Nagaland', 'Mizoram', 'Andaman & Nicobar Islands']\n"
     ]
    }
   ],
   "source": [
    "#scrape state name\n",
    "state=[]\n",
    "try:\n",
    "    names=driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"][1]/tbody/tr/td[2]')\n",
    "    for x in names:\n",
    "        state.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    state.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    state.append('-')\n",
    "state=state[0:33]\n",
    "print(len(state),state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d461cf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['3,108,022', '2,071,286', '1,974,532', '1,962,725', '1,937,066', '1,363,926', '1,218,193', '1,136,137', '1,133,837', '1,128,907', '932,470', '904,642', '870,665', '670,881', '650,302', '614,227', '412,612', '406,416', '358,863', '272,159', '199,917', '176,269', '82,604', '62,550', '45,635', '44,238', '38,785', '37,557', '36,594', '35,124', '31,913', '27,824', '10,371']\n"
     ]
    }
   ],
   "source": [
    "#scrape GSDP(18-19) of state \n",
    "GSDP_18_19=[]\n",
    "try:\n",
    "    gsdp_18_19=driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"][1]/tbody/tr/td[4]')\n",
    "    for x in gsdp_18_19:\n",
    "        GSDP_18_19.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP_18_19.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    GSDP_18_19.append('-')\n",
    "GSDP_18_19=GSDP_18_19[0:33]\n",
    "print(len(GSDP_18_19),GSDP_18_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5e25cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['-', '2,364,514', '2,257,575', '2,241,368', '-', '1,554,992', '1,413,620', '1,322,821', '1,317,728', '1,313,391', '-', '1,043,759', '994,154', '774,869', '751,396', '673,107', '493,167', '457,608', '393,722', '302,621', '227,927', '195,405', '-', '72,636', '-', '-', '42,697', '42,756', '-', '-', '-', '-', '-']\n"
     ]
    }
   ],
   "source": [
    "#scrape GSDP(19-20) of state \n",
    "GSDP_19_20=[]\n",
    "try:\n",
    "    gsdp_19_20=driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"][1]/tbody/tr/td[3]')\n",
    "    for x in gsdp_19_20:\n",
    "        GSDP_19_20.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP_19_20.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    GSDP_19_20.append('-')\n",
    "GSDP_19_20=GSDP_19_20[0:33]\n",
    "print(len(GSDP_19_20),GSDP_19_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be7f3aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['13.24%', '8.82%', '8.41%', '8.36%', '8.25%', '5.81%', '5.19%', '4.84%', '4.83%', '4.81%', '3.97%', '3.85%', '3.71%', '2.86%', '2.77%', '2.62%', '1.76%', '1.73%', '1.53%', '1.16%', '0.85%', '0.75%', '0.35%', '0.27%', '0.19%', '0.19%', '0.17%', '0.16%', '0.16%', '0.15%', '0.14%', '0.12%', '0.04%']\n"
     ]
    }
   ],
   "source": [
    "#scrape Share(2019) of state \n",
    "GSDP_19=[]\n",
    "try:\n",
    "    gsdp_19=driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"][1]/tbody/tr/td[5]')\n",
    "    for x in gsdp_19:\n",
    "        GSDP_19.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP_19.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    GSDP_19.append('-')\n",
    "GSDP_19=GSDP_19[0:33]\n",
    "print(len(GSDP_19),GSDP_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78ab04b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['417.163', '278.011', '265.024', '263.440', '259.996', '183.068', '163.507', '152.494', '152.185', '151.523', '125.157', '121.422', '116.862', '90.047', '87.284', '82.442', '55.381', '54.550', '48.167', '36.530', '26.833', '23.659', '11.087', '8.396', '6.125', '5.938', '5.206', '5.041', '4.912', '4.714', '4.283', '3.735', '1.392']\n"
     ]
    }
   ],
   "source": [
    "#scrape GDP($ billion) of state \n",
    "GDP=[]\n",
    "try:\n",
    "    gsdp=driver.find_elements(By.XPATH,'//table[@class=\"display dataTable\"][1]/tbody/tr/td[6]')\n",
    "    for x in gsdp:\n",
    "        GDP.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    GDP.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    GDP.append('-')\n",
    "GDP=GDP[0:33]\n",
    "print(len(GDP),GDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf537472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State Name</th>\n",
       "      <th>State GSDP(18-19)</th>\n",
       "      <th>State GSDP(19_20)</th>\n",
       "      <th>State Share(2019)</th>\n",
       "      <th>State GDP($Billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>3,108,022</td>\n",
       "      <td>-</td>\n",
       "      <td>13.24%</td>\n",
       "      <td>417.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>2,071,286</td>\n",
       "      <td>2,364,514</td>\n",
       "      <td>8.82%</td>\n",
       "      <td>278.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,974,532</td>\n",
       "      <td>2,257,575</td>\n",
       "      <td>8.41%</td>\n",
       "      <td>265.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,962,725</td>\n",
       "      <td>2,241,368</td>\n",
       "      <td>8.36%</td>\n",
       "      <td>263.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,937,066</td>\n",
       "      <td>-</td>\n",
       "      <td>8.25%</td>\n",
       "      <td>259.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,363,926</td>\n",
       "      <td>1,554,992</td>\n",
       "      <td>5.81%</td>\n",
       "      <td>183.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,218,193</td>\n",
       "      <td>1,413,620</td>\n",
       "      <td>5.19%</td>\n",
       "      <td>163.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>1,136,137</td>\n",
       "      <td>1,322,821</td>\n",
       "      <td>4.84%</td>\n",
       "      <td>152.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>1,133,837</td>\n",
       "      <td>1,317,728</td>\n",
       "      <td>4.83%</td>\n",
       "      <td>152.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>1,128,907</td>\n",
       "      <td>1,313,391</td>\n",
       "      <td>4.81%</td>\n",
       "      <td>151.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>932,470</td>\n",
       "      <td>-</td>\n",
       "      <td>3.97%</td>\n",
       "      <td>125.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>904,642</td>\n",
       "      <td>1,043,759</td>\n",
       "      <td>3.85%</td>\n",
       "      <td>121.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>870,665</td>\n",
       "      <td>994,154</td>\n",
       "      <td>3.71%</td>\n",
       "      <td>116.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>670,881</td>\n",
       "      <td>774,869</td>\n",
       "      <td>2.86%</td>\n",
       "      <td>90.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>650,302</td>\n",
       "      <td>751,396</td>\n",
       "      <td>2.77%</td>\n",
       "      <td>87.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>614,227</td>\n",
       "      <td>673,107</td>\n",
       "      <td>2.62%</td>\n",
       "      <td>82.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>412,612</td>\n",
       "      <td>493,167</td>\n",
       "      <td>1.76%</td>\n",
       "      <td>55.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>406,416</td>\n",
       "      <td>457,608</td>\n",
       "      <td>1.73%</td>\n",
       "      <td>54.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>358,863</td>\n",
       "      <td>393,722</td>\n",
       "      <td>1.53%</td>\n",
       "      <td>48.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>272,159</td>\n",
       "      <td>302,621</td>\n",
       "      <td>1.16%</td>\n",
       "      <td>36.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir-UT</td>\n",
       "      <td>199,917</td>\n",
       "      <td>227,927</td>\n",
       "      <td>0.85%</td>\n",
       "      <td>26.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>176,269</td>\n",
       "      <td>195,405</td>\n",
       "      <td>0.75%</td>\n",
       "      <td>23.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>82,604</td>\n",
       "      <td>-</td>\n",
       "      <td>0.35%</td>\n",
       "      <td>11.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>62,550</td>\n",
       "      <td>72,636</td>\n",
       "      <td>0.27%</td>\n",
       "      <td>8.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>45,635</td>\n",
       "      <td>-</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>6.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>44,238</td>\n",
       "      <td>-</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>5.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>38,785</td>\n",
       "      <td>42,697</td>\n",
       "      <td>0.17%</td>\n",
       "      <td>5.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>37,557</td>\n",
       "      <td>42,756</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>5.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>36,594</td>\n",
       "      <td>-</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>4.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>35,124</td>\n",
       "      <td>-</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>31,913</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>27,824</td>\n",
       "      <td>-</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>10,371</td>\n",
       "      <td>-</td>\n",
       "      <td>0.04%</td>\n",
       "      <td>1.392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                 State Name State GSDP(18-19) State GSDP(19_20)  \\\n",
       "0     1                Maharashtra         3,108,022                 -   \n",
       "1     2                 Tamil Nadu         2,071,286         2,364,514   \n",
       "2     3              Uttar Pradesh         1,974,532         2,257,575   \n",
       "3     4                  Karnataka         1,962,725         2,241,368   \n",
       "4     5                    Gujarat         1,937,066                 -   \n",
       "5     6                West Bengal         1,363,926         1,554,992   \n",
       "6     7                  Rajasthan         1,218,193         1,413,620   \n",
       "7     8             Madhya Pradesh         1,136,137         1,322,821   \n",
       "8     9             Andhra Pradesh         1,133,837         1,317,728   \n",
       "9    10                  Telangana         1,128,907         1,313,391   \n",
       "10   11                     Kerala           932,470                 -   \n",
       "11   12                      Delhi           904,642         1,043,759   \n",
       "12   13                    Haryana           870,665           994,154   \n",
       "13   14                     Odisha           670,881           774,869   \n",
       "14   15                      Bihar           650,302           751,396   \n",
       "15   16                     Punjab           614,227           673,107   \n",
       "16   17                      Assam           412,612           493,167   \n",
       "17   18               Chhattisgarh           406,416           457,608   \n",
       "18   19                  Jharkhand           358,863           393,722   \n",
       "19   20                Uttarakhand           272,159           302,621   \n",
       "20   21         Jammu & Kashmir-UT           199,917           227,927   \n",
       "21   22           Himachal Pradesh           176,269           195,405   \n",
       "22   23                        Goa            82,604                 -   \n",
       "23   24                    Tripura            62,550            72,636   \n",
       "24   25                 Chandigarh            45,635                 -   \n",
       "25   26                 Puducherry            44,238                 -   \n",
       "26   27                  Meghalaya            38,785            42,697   \n",
       "27   28                     Sikkim            37,557            42,756   \n",
       "28   29                    Manipur            36,594                 -   \n",
       "29   30          Arunachal Pradesh            35,124                 -   \n",
       "30   31                   Nagaland            31,913                 -   \n",
       "31   32                    Mizoram            27,824                 -   \n",
       "32   33  Andaman & Nicobar Islands            10,371                 -   \n",
       "\n",
       "   State Share(2019) State GDP($Billions)  \n",
       "0             13.24%              417.163  \n",
       "1              8.82%              278.011  \n",
       "2              8.41%              265.024  \n",
       "3              8.36%              263.440  \n",
       "4              8.25%              259.996  \n",
       "5              5.81%              183.068  \n",
       "6              5.19%              163.507  \n",
       "7              4.84%              152.494  \n",
       "8              4.83%              152.185  \n",
       "9              4.81%              151.523  \n",
       "10             3.97%              125.157  \n",
       "11             3.85%              121.422  \n",
       "12             3.71%              116.862  \n",
       "13             2.86%               90.047  \n",
       "14             2.77%               87.284  \n",
       "15             2.62%               82.442  \n",
       "16             1.76%               55.381  \n",
       "17             1.73%               54.550  \n",
       "18             1.53%               48.167  \n",
       "19             1.16%               36.530  \n",
       "20             0.85%               26.833  \n",
       "21             0.75%               23.659  \n",
       "22             0.35%               11.087  \n",
       "23             0.27%                8.396  \n",
       "24             0.19%                6.125  \n",
       "25             0.19%                5.938  \n",
       "26             0.17%                5.206  \n",
       "27             0.16%                5.041  \n",
       "28             0.16%                4.912  \n",
       "29             0.15%                4.714  \n",
       "30             0.14%                4.283  \n",
       "31             0.12%                3.735  \n",
       "32             0.04%                1.392  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make dataframe\n",
    "df=pd.DataFrame()\n",
    "df['Rank']=rank\n",
    "df['State Name']=state\n",
    "df['State GSDP(18-19)']=GSDP_18_19\n",
    "df['State GSDP(19_20)']=GSDP_19_20\n",
    "df['State Share(2019)']=GSDP_19\n",
    "df['State GDP($Billions)']=GDP\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b523c",
   "metadata": {},
   "source": [
    "### 4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fe186d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://github.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d41cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_link=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "try:\n",
    "    trending_link.click()\n",
    "except ElementNotInteractableException:#handling element not clickable exception\n",
    "    driver.get(trending_link.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3f8a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['cloudflare', 'Lissy93', 'polyfillpolyfill', 'HumanAIGC', 'cloudcommunity', 'yuzu-emu', 'microsoft', 'HumanAIGC', 'kyegomez', 'dockur', 'FuelLabs', 'dair-ai', '1c7', 'memorysafety', 'Avaiga', 'aishwaryanr', 'wazuh', 'myshell-ai', 'pure-admin', 'python-poetry', 'microsoft', 'redis', 'FuelLabs', 'ossu', 'embedchain']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Scrape Repository Title\n",
    "title=[]\n",
    "try:\n",
    "    titles=driver.find_elements(By.XPATH,'//span[@class=\"text-normal\"]')\n",
    "    for x in titles:\n",
    "        title.append(x.text.replace(' /',''))\n",
    "except NoSuchElementException:\n",
    "    title.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    title.append('-')\n",
    "print(len(title),title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d96e0c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 ['A library for building fast, reliable and evolvable network services.', '🕵️\\u200d♂️ All-in-one OSINT tool for analysing any website', 'Automatic polyfill service.', 'A curated list of free courses & certifications.', 'Nintendo Switch emulator', '18 Lessons, Get Started Building with Generative AI 🔗 https://microsoft.github.io/generative-ai-for-beginners/', 'Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation', 'Implementation of \"BitNet: Scaling 1-bit Transformers for Large Language Models\" in pytorch', 'Windows in a Docker container.', 'Rust full node implementation of the Fuel v2 protocol.', '🔥Highlighting the top ML papers every week.', '👩🏿\\u200d💻👨🏾\\u200d💻👩🏼\\u200d💻👨🏽\\u200d💻👩🏻\\u200d💻中国独立开发者项目列表 -- 分享大家都在做什么', 'This repository is the future home of the River reverse proxy application, based on the pingora library from Cloudflare.', 'Turns Data and AI algorithms into production-ready web applications in no time.', 'A one stop repository for generative AI research updates, interview resources, notebooks and much more!', 'Wazuh - The Open Source Security Platform. Unified XDR and SIEM protection for endpoints and cloud workloads.', 'High-quality multi-lingual text-to-speech library by MyShell.ai. Support English, Spanish, French, Chinese, Japanese and Korean.', '🔥 全面ESM+Vue3+Vite+Element-Plus+TypeScript编写的一款后台管理系统（兼容移动端）', 'Python packaging and dependency management made easy', 'Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities', '🚀 A robust, performance-focused, and full-featured Redis client for Node.js.', 'Fuel Network Rust SDK', '🎓 Path to a free self-taught education in Computer Science!', 'The Open Source RAG framework']\n"
     ]
    }
   ],
   "source": [
    "#scrape Repository Description\n",
    "description=[]\n",
    "try:\n",
    "    descri=driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/p')\n",
    "    for x in descri:\n",
    "        description.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    description.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    description.append('-')\n",
    "print(len(description),description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fad8ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['12,811', '14,690', '6,597', '4,943', '19,268', '38,622', '25,500', '13,468', '864', '3,998', '45,144', '7,436', '29,850', '515', '6,896', '972', '8,668', '1,189', '12,794', '28,819', '17,557', '13,340', '35,647', '159,852', '8,020']\n"
     ]
    }
   ],
   "source": [
    "#scarpe Contributors count\n",
    "contributor=[]\n",
    "try:\n",
    "    contributors=driver.find_elements(By.XPATH,'//div[@class=\"f6 color-fg-muted mt-2\"]/a[1]')\n",
    "    for x in contributors:\n",
    "        contributor.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    contributor.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    contributor.append('-')\n",
    "print(len(contributor),contributor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637413c",
   "metadata": {},
   "source": [
    "### 5. Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https://www.billboard.com/\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "1 Song name\n",
    "2 Artist name\n",
    "3 Last week rank\n",
    "4 Peak rank\n",
    "5 Weeks on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6d66588",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.billboard.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dab4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open charts\n",
    "charts=driver.find_element(By.XPATH,'/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "try:\n",
    "    charts.click()\n",
    "except ElementNotInteractableException:#handling element not clickable exception\n",
    "    driver.get(charts.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36047a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open Hot 100 page\n",
    "hot=driver.find_element(By.XPATH,'/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div[1]/div[2]/span/a')\n",
    "try:\n",
    "    hot.click()\n",
    "except ElementNotInteractableException:#handling element not clickable exception\n",
    "    driver.get(hot.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86ebee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [\"Texas Hold 'Em\", 'Lovin On Me', 'Lose Control', 'Carnival', 'Beautiful Things', 'Snooze', 'Cruel Summer', 'Greedy', 'I Remember Everything', 'Agora Hills', 'Yes, And?', 'Stick Season', 'Fast Car', 'Water', 'Last Night', 'Redrum', \"Thinkin' Bout Me\", \"Is It Over Now? (Taylor's Version) [From The Vault]\", 'Pretty Little Poison', 'Flowers', 'Paint The Town Red', 'Houdini', 'Made For Me', 'Never Lose Me', 'La Diabla', 'Where The Wild Things Are', 'Training Season', 'The Painter', 'Feather', 'What Was I Made For?', 'Rich Baby Daddy', 'Whatever She Wants', 'Truck Bed', 'Selfish', 'Everybody', 'Vampire', 'Wild Ones', 'On My Mama', 'Praise Jah In The Moonlight', 'Save Me', 'Exes', 'Breathe', 'Dance The Night', 'Need A Favor', 'Surround Sound', 'World On Fire', 'Yeah!', 'Burn It Down', 'Good Good', 'La Victima', 'End Of Beginning', 'Get In With Me', 'Burn', 'Man Made A Bar', '16 Carriages', 'First Person Shooter', 'Fuk Sumn', 'One Of The Girls', 'Back To Me', 'One Call', 'Contigo', '23', 'Murder On The Dancefloor', 'You Broke My Heart', 'Think U The Shit (Fart)', 'FTCU', 'FE!N', 'Hiss', 'I Can Feel It', 'Bandit', 'Spin You Around (1/24)', 'Soak City', 'Forever', \"Mamaw's House\", 'Act II: Date @ 8', 'Igual Que Un Angel', 'Nee-nah', 'Wildflowers And Wild Horses', 'Bellakeo', 'Harley Quinn', 'Standing Next To You', 'Bittersweet', 'IDGAF', 'Home', 'Vultures', 'Yeah Glo!', 'Coal', 'Mmhmm', 'Psycho CEO', 'Perro Negro', 'Sensational', 'Oklahoma Smokeshow', 'Tu Name', 'Do It', 'Worth It', 'Talking', 'Monaco', 'Where It Ends', 'Wondering Why', 'Northern Attitude']\n"
     ]
    }
   ],
   "source": [
    "#scrape song name\n",
    "name=[]\n",
    "try:\n",
    "    names=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li/h3')\n",
    "    for x in names:\n",
    "        name.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    name.append('-')\n",
    "print(len(name),name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad148e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Beyonce', 'Jack Harlow', 'Teddy Swims', '¥$: Kanye West & Ty Dolla $ign Featuring Rich The Kid & Playboi Carti', 'Benson Boone', 'SZA', 'Taylor Swift', 'Tate McRae', 'Zach Bryan Featuring Kacey Musgraves', 'Doja Cat', 'Ariana Grande', 'Noah Kahan', 'Luke Combs', 'Tyla', 'Morgan Wallen', '21 Savage', 'Morgan Wallen', 'Taylor Swift', 'Warren Zeiders', 'Miley Cyrus', 'Doja Cat', 'Dua Lipa', 'Muni Long', 'Flo Milli', 'Xavi', 'Luke Combs', 'Dua Lipa', 'Cody Johnson', 'Sabrina Carpenter', 'Billie Eilish', 'Drake Featuring Sexyy Red & SZA', 'Bryson Tiller', 'HARDY', 'Justin Timberlake', 'Nicki Minaj Featuring Lil Uzi Vert', 'Olivia Rodrigo', 'Jessie Murph & Jelly Roll', 'Victoria Monet', 'YG Marley', 'Jelly Roll With Lainey Wilson', 'Tate McRae', 'Yeat', 'Dua Lipa', 'Jelly Roll', 'JID Featuring 21 Savage & Baby Tate', 'Nate Smith', 'Usher Featuring Lil Jon & Ludacris', 'Parker McCollum', 'Usher, Summer Walker & 21 Savage', 'Xavi', 'Djo', 'BossMan Dlow', '¥$: Kanye West & Ty Dolla $ign', 'Morgan Wallen Featuring Eric Church', 'Beyonce', 'Drake Featuring J. Cole', '¥$: Kanye West & Ty Dolla $ign', 'The Weeknd, Jennie & Lily Rose Depp', '¥$: Kanye West & Ty Dolla $ign', 'Rich Amiri', 'Karol G & Tiesto', 'Chayce Beckham', 'Sophie Ellis-Bextor', 'Drake', 'Ice Spice', 'Nicki Minaj', 'Travis Scott Featuring Playboi Carti', 'Megan Thee Stallion', 'Kane Brown', 'Don Toliver', 'Morgan Wallen', '310babii', 'Noah Kahan', 'Thomas Rhett Featuring Morgan Wallen', '4Batz', 'Kali Uchis & Peso Pluma', '21 Savage, Travis Scott & Metro Boomin', 'Lainey Wilson', 'Peso Pluma & Anitta', 'Fuerza Regida & Marshmello', 'Jung Kook', 'Gunna', 'Drake Featuring Yeat', 'Good Neighbours', '¥$: Kanye West & Ty Dolla $ign Featuring Lil Durk & Bump J', 'GloRilla', 'Dylan Gossett', 'BigXthaPlug', 'Yeat', 'Bad Bunny & Feid', 'Chris Brown Featuring Davido & Lojay', 'Zach Bryan', 'Fuerza Regida', '¥$: Kanye West & Ty Dolla $ign', 'Offset & Don Toliver', '¥$: Kanye West & Ty Dolla $ign Featuring North West', 'Bad Bunny', 'Bailey Zimmerman', 'The Red Clay Strays', 'Noah Kahan With Hozier']\n"
     ]
    }
   ],
   "source": [
    "#scrape Artist name\n",
    "artist=[]\n",
    "try:\n",
    "    artists=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[1]/span[1]')\n",
    "    for x in artists:\n",
    "        artist.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    artist.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    artist.append('-')\n",
    "print(len(artist),artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "792ec106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['2', '1', '5', '3', '4', '7', '6', '9', '8', '12', '31', '10', '11', '13', '14', '15', '16', '18', '24', '17', '19', '25', '32', '21', '22', '29', '-', '35', '42', '27', '41', '-', '51', '37', '40', '43', '45', '44', '63', '47', '49', '-', '46', '50', '54', '56', '20', '60', '36', '58', '-', '59', '33', '66', '38', '57', '23', '72', '26', '68', '-', '77', '73', '74', '62', '71', '76', '48', '82', '78', '70', '84', '28', '83', '81', '69', '80', '-', '87', '85', '61', '-', '88', '-', '34', '89', '95', '96', '-', '99', '-', '-', '-', '52', '-', '30', '97', '-', '-', '75']\n"
     ]
    }
   ],
   "source": [
    "#scrape Last week rank\n",
    "rank=[]\n",
    "try:\n",
    "    ranks=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[4]/span')\n",
    "    for x in ranks:\n",
    "        rank.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    rank.append('-')\n",
    "print(len(rank),rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e46e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['1', '1', '2', '3', '3', '2', '1', '3', '1', '7', '1', '10', '2', '7', '1', '5', '7', '1', '19', '1', '1', '11', '23', '18', '20', '26', '27', '28', '29', '14', '11', '32', '33', '19', '24', '1', '35', '33', '39', '19', '34', '42', '6', '13', '40', '21', '1', '48', '25', '46', '51', '52', '33', '15', '38', '1', '23', '57', '26', '60', '61', '62', '51', '11', '37', '15', '5', '1', '59', '38', '24', '61', '28', '55', '59', '22', '10', '78', '53', '40', '5', '82', '2', '77', '34', '86', '86', '65', '89', '20', '91', '72', '93', '52', '92', '30', '5', '32', '71', '37']\n"
     ]
    }
   ],
   "source": [
    "#scrape Peak rank\n",
    "peak=[]\n",
    "try:\n",
    "    peaks=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[5]/span')\n",
    "    for x in peaks:\n",
    "        peak.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    peak.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    peak.append('-')\n",
    "print(len(peak),peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ae9e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['2', '15', '28', '2', '5', '62', '42', '23', '26', '22', '6', '21', '48', '21', '56', '6', '51', '17', '24', '53', '29', '15', '6', '10', '10', '14', '1', '20', '12', '31', '20', '1', '36', '4', '11', '31', '20', '23', '4', '33', '14', '1', '34', '45', '15', '17', '47', '18', '28', '10', '1', '3', '2', '22', '2', '20', '2', '9', '2', '4', '1', '8', '8', '14', '4', '11', '17', '4', '8', '3', '4', '10', '2', '8', '7', '6', '6', '4', '10', '16', '16', '1', '20', '3', '2', '2', '7', '9', '1', '16', '3', '17', '1', '2', '5', '2', '19', '8', '8', '12']\n"
     ]
    }
   ],
   "source": [
    "#scrape Weeks on board\n",
    "board=[]\n",
    "try:\n",
    "    boards=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[6]/span')\n",
    "    for x in boards:\n",
    "        board.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    board.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    board.append('-')\n",
    "print(len(board),board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46f7fd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song name</th>\n",
       "      <th>Artist name</th>\n",
       "      <th>Last week rank</th>\n",
       "      <th>Peak rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Texas Hold 'Em</td>\n",
       "      <td>Beyonce</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lovin On Me</td>\n",
       "      <td>Jack Harlow</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lose Control</td>\n",
       "      <td>Teddy Swims</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carnival</td>\n",
       "      <td>¥$: Kanye West &amp; Ty Dolla $ign Featuring Rich ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beautiful Things</td>\n",
       "      <td>Benson Boone</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Talking</td>\n",
       "      <td>¥$: Kanye West &amp; Ty Dolla $ign Featuring North...</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Monaco</td>\n",
       "      <td>Bad Bunny</td>\n",
       "      <td>97</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Where It Ends</td>\n",
       "      <td>Bailey Zimmerman</td>\n",
       "      <td>-</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Wondering Why</td>\n",
       "      <td>The Red Clay Strays</td>\n",
       "      <td>-</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Northern Attitude</td>\n",
       "      <td>Noah Kahan With Hozier</td>\n",
       "      <td>75</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Song name                                        Artist name  \\\n",
       "0      Texas Hold 'Em                                            Beyonce   \n",
       "1         Lovin On Me                                        Jack Harlow   \n",
       "2        Lose Control                                        Teddy Swims   \n",
       "3            Carnival  ¥$: Kanye West & Ty Dolla $ign Featuring Rich ...   \n",
       "4    Beautiful Things                                       Benson Boone   \n",
       "..                ...                                                ...   \n",
       "95            Talking  ¥$: Kanye West & Ty Dolla $ign Featuring North...   \n",
       "96             Monaco                                          Bad Bunny   \n",
       "97      Where It Ends                                   Bailey Zimmerman   \n",
       "98      Wondering Why                                The Red Clay Strays   \n",
       "99  Northern Attitude                             Noah Kahan With Hozier   \n",
       "\n",
       "   Last week rank Peak rank Weeks on board  \n",
       "0               2         1              2  \n",
       "1               1         1             15  \n",
       "2               5         2             28  \n",
       "3               3         3              2  \n",
       "4               4         3              5  \n",
       "..            ...       ...            ...  \n",
       "95             30        30              2  \n",
       "96             97         5             19  \n",
       "97              -        32              8  \n",
       "98              -        71              8  \n",
       "99             75        37             12  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make dataframe\n",
    "df=pd.DataFrame()\n",
    "df['Song name']=name\n",
    "df['Artist name']=artist\n",
    "df['Last week rank']=rank\n",
    "df['Peak rank']=peak\n",
    "df['Weeks on board']=board\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b91e66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160decfe",
   "metadata": {},
   "source": [
    "### 6. Scrape the details of Highest selling novels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f423471",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Fetching url\n",
    "driver.get('https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8fad74de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volumes Sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name       Author Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes Sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Book = []\n",
    "Author = []\n",
    "Publisher = []\n",
    "Volume = []\n",
    "Genre =[]\n",
    "\n",
    "\n",
    "# for scraping the book name\n",
    "book_tags = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[2]')\n",
    "for i in book_tags:\n",
    "    book= i.text\n",
    "    Book.append(book)\n",
    "    \n",
    "# for scraping the author name \n",
    "name_tags = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[3]')\n",
    "for i in name_tags:\n",
    "    name= i.text\n",
    "    Author.append(name)\n",
    "    \n",
    "# for scraping the volumes sold\n",
    "sold_tags = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[4]')\n",
    "for i in sold_tags:\n",
    "    sold= i.text\n",
    "    Volume.append(sold)\n",
    "    \n",
    "# for scraping the publisher \n",
    "publish_tags = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[5]')\n",
    "for i in publish_tags:\n",
    "    publish= i.text\n",
    "    Publisher.append(publish)\n",
    "    \n",
    "# for scraping the genre \n",
    "genre_tags = driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[6]')\n",
    "for i in genre_tags:\n",
    "    genre= i.text\n",
    "    Genre.append(genre)\n",
    "    \n",
    "# Creating the data frame\n",
    "import pandas as pd \n",
    "df= pd.DataFrame({'Book Name':Book,'Author Name':Author,\"Volumes Sold\":Volume,'Publisher':Publisher,'Genre':Genre})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d750e",
   "metadata": {},
   "source": [
    "### 6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e3574e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97f6952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]\n"
     ]
    }
   ],
   "source": [
    "#scrape book name\n",
    "name=[]\n",
    "try:\n",
    "    names=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[2]')\n",
    "    for x in names:\n",
    "        name.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    name.append('-')\n",
    "print(len(name),name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7c5bef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie']\n"
     ]
    }
   ],
   "source": [
    "#scrape Author name\n",
    "author=[]\n",
    "try:\n",
    "    authors=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[3]')\n",
    "    for x in authors:\n",
    "        author.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    author.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    author.append('-')\n",
    "print(len(author),author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc6aca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095']\n"
     ]
    }
   ],
   "source": [
    "#scrape Volumes sold\n",
    "sold=[]\n",
    "try:\n",
    "    solds=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[4]')\n",
    "    for x in solds:\n",
    "        sold.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    sold.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    sold.append('-')\n",
    "print(len(sold),sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98d6d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin']\n"
     ]
    }
   ],
   "source": [
    "#scrape publisher name\n",
    "publisher=[]\n",
    "try:\n",
    "    publishers=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[5]')\n",
    "    for x in publishers:\n",
    "        publisher.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    publisher.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    publisher.append('-')\n",
    "print(len(publisher),publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2f4f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General']\n"
     ]
    }
   ],
   "source": [
    "#scrape Genre name\n",
    "genre=[]\n",
    "try:\n",
    "    genres=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[6]')\n",
    "    for x in genres:\n",
    "        genre.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    genre.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    genre.append('-')\n",
    "print(len(genre),genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a96f445e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book name</th>\n",
       "      <th>Author name</th>\n",
       "      <th>Volumes sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book name       Author name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make dataframe\n",
    "df=pd.DataFrame()\n",
    "df['Book name']=name\n",
    "df['Author name']=author\n",
    "df['Volumes sold']=sold\n",
    "df['Publisher']=publisher\n",
    "df['Genre']=genre\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e60bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcb5a9",
   "metadata": {},
   "source": [
    "### 7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c80817ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Fetching url\n",
    "driver.get('https://www.imdb.com/list/ls512407256/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b8f3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=[]\n",
    "Year = []\n",
    "Genre = []\n",
    "Runtime = []\n",
    "Rating = []\n",
    "Votes= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0cadf270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>55 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,262,355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2025)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,320,331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,072,174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>313,460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>273,387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>True Detective</td>\n",
       "      <td>(2014– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>55 min</td>\n",
       "      <td>8.9</td>\n",
       "      <td>645,444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Teen Wolf</td>\n",
       "      <td>(2011–2017)</td>\n",
       "      <td>Action, Drama, Fantasy</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>162,050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The OA</td>\n",
       "      <td>(2016–2019)</td>\n",
       "      <td>Drama, Fantasy, Mystery</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>114,868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The Simpsons</td>\n",
       "      <td>(1989– )</td>\n",
       "      <td>Animation, Comedy</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>433,070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Desperate Housewives</td>\n",
       "      <td>(2004–2012)</td>\n",
       "      <td>Comedy, Drama, Mystery</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>138,683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name    Year Span                     Genre Run Time  \\\n",
       "0        Game of Thrones  (2011–2019)  Action, Adventure, Drama   55 min   \n",
       "1        Stranger Things  (2016–2025)    Drama, Fantasy, Horror   51 min   \n",
       "2       The Walking Dead  (2010–2022)   Drama, Horror, Thriller   44 min   \n",
       "3         13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   60 min   \n",
       "4                The 100  (2014–2020)    Drama, Mystery, Sci-Fi   43 min   \n",
       "..                   ...          ...                       ...      ...   \n",
       "95        True Detective     (2014– )     Crime, Drama, Mystery   55 min   \n",
       "96             Teen Wolf  (2011–2017)    Action, Drama, Fantasy   41 min   \n",
       "97                The OA  (2016–2019)   Drama, Fantasy, Mystery   60 min   \n",
       "98          The Simpsons     (1989– )         Animation, Comedy   22 min   \n",
       "99  Desperate Housewives  (2004–2012)    Comedy, Drama, Mystery   45 min   \n",
       "\n",
       "   Rating      Votes  \n",
       "0     9.2  2,262,355  \n",
       "1     8.7  1,320,331  \n",
       "2     8.1  1,072,174  \n",
       "3     7.5    313,460  \n",
       "4     7.6    273,387  \n",
       "..    ...        ...  \n",
       "95    8.9    645,444  \n",
       "96    7.7    162,050  \n",
       "97    7.8    114,868  \n",
       "98    8.7    433,070  \n",
       "99    7.6    138,683  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for scraping the name on the page\n",
    "name_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/h3/a')\n",
    "for i in name_tags:\n",
    "    naming = i.text\n",
    "    Name.append(naming)\n",
    "    \n",
    "# for scraping the Year span\n",
    "year_tags = driver.find_elements(By.XPATH,'//span[@class=\"lister-item-year text-muted unbold\"]')\n",
    "for i in year_tags:\n",
    "    year = i.text\n",
    "    Year.append(year)\n",
    "    \n",
    "# for scraping the Genre\n",
    "genre_tags = driver.find_elements(By.XPATH,'//span[@class=\"genre\"]')\n",
    "for i in genre_tags:\n",
    "    genre = i.text\n",
    "    Genre.append(genre)\n",
    "    \n",
    "# for scraping the Runtime\n",
    "time_tags = driver.find_elements(By.XPATH,'//span[@class=\"runtime\"]')\n",
    "for i in time_tags:\n",
    "    time= i.text\n",
    "    Runtime.append(time)\n",
    "    \n",
    "# for scraping the Rating\n",
    "rating_tags = driver.find_elements(By.XPATH,'//div[@class=\"ipl-rating-widget\"]/div/span[2]')\n",
    "for i in rating_tags:\n",
    "    rating= i.text\n",
    "    Rating.append(rating)\n",
    "    \n",
    "# for scraping the Voting\n",
    "voting_tags = driver.find_elements(By.XPATH,'//div[@class=\"lister-item-content\"]/p[4]/span[2]')\n",
    "for i in voting_tags:\n",
    "    voting= i.text\n",
    "    Votes.append(voting)\n",
    "    \n",
    "# Creating dataframe\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({\"Name\":Name,\"Year Span\":Year,\"Genre\":Genre,\"Run Time\":Runtime,\"Rating\":Rating,\"Votes\":Votes})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda9a6e",
   "metadata": {},
   "source": [
    "### 8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b396b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Fetching url\n",
    "driver.get('https://archive.ics.uci.edu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e2d58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open view all datasets\n",
    "data_sets=driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')\n",
    "try:\n",
    "    data_sets.click()\n",
    "except ElementNotInteractableException:#handling element not clickable exception\n",
    "    driver.get(data_sets.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86f13306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['Iris', 'Dry Bean Dataset', 'Heart Disease', 'Rice (Cammeo and Osmancik)', 'Adult', 'Raisin', 'Breast Cancer Wisconsin (Diagnostic)', 'Wine', 'Wine Quality', 'Diabetes']\n"
     ]
    }
   ],
   "source": [
    "#scrape Dataset name\n",
    "name=[]\n",
    "try:\n",
    "    names=driver.find_elements(By.XPATH,'//h2[@class=\"truncate text-primary\"]/a')\n",
    "    for x in names:\n",
    "        name.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    name.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    name.append('-')\n",
    "print(len(name),name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c308a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['Tabular', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate, Time-Series']\n"
     ]
    }
   ],
   "source": [
    "#scrape Data type\n",
    "data=[]\n",
    "try:\n",
    "    datas=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[2]/span')\n",
    "    for x in datas:\n",
    "        data.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    data.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    data.append('-')\n",
    "print(len(data),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e80403b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Classification']\n"
     ]
    }
   ],
   "source": [
    "#scrape Task\n",
    "task=[]\n",
    "try:\n",
    "    tasks=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[1]/span')\n",
    "    for x in tasks:\n",
    "        task.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    task.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    task.append('-')\n",
    "print(len(task),task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26878ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['150 Instances', '13.61K Instances', '303 Instances', '3.81K Instances', '48.84K Instances', '900 Instances', '569 Instances', '178 Instances', '4.9K Instances', '1 Instances']\n"
     ]
    }
   ],
   "source": [
    "#scrape No of instances\n",
    "instance=[]\n",
    "try:\n",
    "    instances=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[3]/span')\n",
    "    for x in instances:\n",
    "        instance.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    instance.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    instance.append('-')\n",
    "print(len(instance),instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "274cb805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['4 Features', '16 Features', '13 Features', '7 Features', '14 Features', '8 Features', '30 Features', '13 Features', '12 Features', '20 Features']\n"
     ]
    }
   ],
   "source": [
    "#scrape No of attribute\n",
    "attribute=[]\n",
    "try:\n",
    "    attributes=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]/div[4]/span')\n",
    "    for x in attributes:\n",
    "        attribute.append(x.text)\n",
    "except NoSuchElementException:\n",
    "    attribute.append('-')\n",
    "except StaleElementReferenceException:\n",
    "    attribute.append('-')\n",
    "print(len(attribute),attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "239a8f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset name</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Task</th>\n",
       "      <th>No of instances</th>\n",
       "      <th>No of attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>150 Instances</td>\n",
       "      <td>4 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>13.61K Instances</td>\n",
       "      <td>16 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>303 Instances</td>\n",
       "      <td>13 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>3.81K Instances</td>\n",
       "      <td>7 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>48.84K Instances</td>\n",
       "      <td>14 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Raisin</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>900 Instances</td>\n",
       "      <td>8 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>569 Instances</td>\n",
       "      <td>30 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Tabular</td>\n",
       "      <td>Classification</td>\n",
       "      <td>178 Instances</td>\n",
       "      <td>13 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wine Quality</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>4.9K Instances</td>\n",
       "      <td>12 Features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Classification</td>\n",
       "      <td>1 Instances</td>\n",
       "      <td>20 Features</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Dataset name                  Data type  \\\n",
       "0                                  Iris                    Tabular   \n",
       "1                      Dry Bean Dataset               Multivariate   \n",
       "2                         Heart Disease               Multivariate   \n",
       "3            Rice (Cammeo and Osmancik)               Multivariate   \n",
       "4                                 Adult               Multivariate   \n",
       "5                                Raisin               Multivariate   \n",
       "6  Breast Cancer Wisconsin (Diagnostic)               Multivariate   \n",
       "7                                  Wine                    Tabular   \n",
       "8                          Wine Quality               Multivariate   \n",
       "9                              Diabetes  Multivariate, Time-Series   \n",
       "\n",
       "                         Task   No of instances No of attribute  \n",
       "0              Classification     150 Instances      4 Features  \n",
       "1              Classification  13.61K Instances     16 Features  \n",
       "2              Classification     303 Instances     13 Features  \n",
       "3              Classification   3.81K Instances      7 Features  \n",
       "4              Classification  48.84K Instances     14 Features  \n",
       "5              Classification     900 Instances      8 Features  \n",
       "6              Classification     569 Instances     30 Features  \n",
       "7              Classification     178 Instances     13 Features  \n",
       "8  Classification, Regression    4.9K Instances     12 Features  \n",
       "9              Classification       1 Instances     20 Features  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make dataframe\n",
    "df=pd.DataFrame()\n",
    "df['Dataset name']=name\n",
    "df['Data type']=data\n",
    "df['Task']=task\n",
    "df['No of instances']=instance\n",
    "df['No of attribute']=attribute\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6842e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
